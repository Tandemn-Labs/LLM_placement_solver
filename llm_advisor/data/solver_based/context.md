## Solver-Based Evaluation Context

This `data.csv` contains solver-based throughput/cost estimates generated by the analytical placement/evaluation pipeline in `solver.py`, not measurements from real hardware runs. The data is produced by running `run-eval-sweep.sh`, which calls `solver.py` in `--evaluate-throughput` mode and writes placement metrics into per-run output directories.

### How the Numbers Are Generated
- `run-eval-sweep.sh` enumerates instance families, TP degrees, PP stages, and fixed IO lengths/batch sizes, then calls `solver.py --evaluate-throughput` for each combination. The script reads instance families from `config/gpu_pool.csv` and GPU counts/pricing metadata from `config/cloud_instances_specs.csv`, uses `config/large` as the model configuration, sets `workload_phase=aggregated`, and fixes `io_length_pairs` and `batch_size_pairs` as declared in the script.  
\n```5:164:/mnt/projects/LLM_placement_solver/run-eval-sweep.sh
# Sweep throughput-only evaluation mode for different instance families, TP, PP, and IO lengths.
# This script DOES NOT run the solver optimization; it only uses --evaluate-throughput.
...
config_dir_list=(
  "${CONFIG_ROOT}/large"
)
workload_phase_list=("aggregated")
io_length_pairs=("2048 512")
batch_size_pairs=("32 32")
cloud_provider="AWS"
...
python3 "${SOLVER}" \
  --config-dir "${config_dir}" \
  --eval-output-root "${eval_output_root}" \
  --cloud-provider "${cloud_provider}" \
  --sequence-length "${input_token_length}" \
  --workload-phase "${workload_phase}" \
  --output-length "${output_token_length}" \
  --min-batch-size "${min_batch_size}" \
  --max-batch-size "${max_batch_size}" \
  --evaluate-throughput \
  --eval-instance-family "${instance_family}" \
  --eval-tp-degree "${tp_degree}" \
  --eval-pp-stages "${pp_stages}"
\n```

- In `solver.py`, evaluation mode builds a placement by evenly splitting the model layers across the requested PP stages, fixes the TP degree per stage, then computes throughput/cost for that placement via `evaluate_manual_placement` (no Gurobi optimization in eval mode).  
\n```4656:4743:/mnt/projects/LLM_placement_solver/solver.py
        if args.evaluate_throughput:
            ...
            solver = LLMPlacementSolverWithTP(
                args.config_dir,
                tp_configuration=None,
                **solver_kwargs,
                skip_gurobi=True
            )
            ...
            for idx in range(pp_stages):
                segment_size = base + (1 if idx < remainder else 0)
                ...
                stages.append({
                    "gpu_type": matching_instances[idx],
                    "tp_degree": args.eval_tp_degree,
                    "gpu_ids": list(range(args.eval_tp_degree)),
                    "start_layer": start_layer,
                    "end_layer": end_layer
                })
            placement = {
                "batch_size": args.min_batch_size,
                "sequence_length": args.sequence_length,
                "output_length": args.output_length,
                "workload_phase": args.workload_phase,
                "stages": stages
            }
            solver.evaluate_manual_placement(placement)
            ...
            solver.save_solution_csv(csv_file)
\n```

### Model Configuration Used
The evaluation uses `config/large/config.csv`, which sets the model topology and solver parameters (e.g., 80-layer Llama3-70B, FP16 bytes per element, max pipeline stages, efficiency factors).  
\n```1:27:/mnt/projects/LLM_placement_solver/config/large/config.csv
parameter,value
model_name,llama3-70b
num_decoder_layers,80
d_model,8192
d_hidden,28672
vocab_size,128256
num_attention_heads,64
layer_weight_memory_gb,1.75
time_limit_seconds,300
optimality_gap,0.01
bytes_per_element,2
enable_segment_quantization,true
max_pipeline_stages,8
min_layers_per_stage,5
min_memory_utilization,0.5
network_bandwidth_percentile_threshold,0.1
optimization_priority,cost_first
max_hourly_cost,999.0
max_cost_per_million_token,100
max_total_cost,999999.0
max_total_runtime_hours,999999.0
throughput_normalization,10000.0
cost_normalization,1.0
total_tokens_to_process,1000000
real_world_efficiency,0.30
micro_batch_size,8
num_kv_heads,8
\n```

### Hardware/Cluster Model (Analytical, Not Measured)
- **Instance pool:** `config/gpu_pool.csv` enumerates available instance families and counts.  
\n```1:11:/mnt/projects/LLM_placement_solver/config/gpu_pool.csv
instance_name,count
p4de.24xlarge,1
p3dn.24xlarge,4
g6e.48xlarge,4
g6e.24xlarge,4
g6e.12xlarge,4
g6e.4xlarge,4
g5.48xlarge,4
g5.24xlarge,4
g5.12xlarge,4
g5.4xlarge,4
\n```
- **Pricing + device specs:** `config/cloud_instances_specs.csv` is used to map instance families to GPU type, GPU count, memory, and hourly price (AWS selected in `run-eval-sweep.sh`).  
\n```1:28:/mnt/projects/LLM_placement_solver/config/cloud_instances_specs.csv
Cloud Provider,Instance Name,GPU Model,GPU Count,...,Price per Hour USD,...,num_gpus,vram_gb_per_gpu,total_vram_gb
AWS,p4de.24xlarge,NVIDIA A100,8,...,$40.96,...,8.0,80.0,640.0
AWS,p3dn.24xlarge,NVIDIA V100,8,...,$31.21,...,8.0,32.0,256.0
AWS,g6e.48xlarge,NVIDIA L40S,8,...,$27.39,...,8.0,48.0,384.0
AWS,g6e.4xlarge,NVIDIA L40S,1,...,$3.42,...,1.0,48.0,48.0
AWS,g5.48xlarge,NVIDIA A10G,8,...,$16.29,...,8.0,24.0,192.0
AWS,g5.12xlarge,NVIDIA A10G,4,...,$5.67,...,4.0,24.0,96.0
\n```
- **Network model:** `config/network_bandwidth.csv` provides a full GPU-to-GPU bandwidth matrix (GB/s) that the solver uses to estimate inter-stage communication and TP collective costs.  

### Throughput & Cost Model
The solver is a roofline-based analytical model with explicit TP/PP and network overheads:
- Per-stage compute throughput is derived from GPU TFLOPS and memory bandwidth, with a batch efficiency factor and TP communication overheads (all-reduce/all-scatter).  
\n```135:704:/mnt/projects/LLM_placement_solver/solver.py
class ThroughputFunctions:
    GPU_SPECS = { ... }
    ...
    def gpu_throughput_with_tp(...):
        """
        GPU throughput with tensor parallelism using roofline model.
        - Prefill: O(nÂ²) attention
        - Decode: O(n) attention
        - Aggregated: prefill+decode combined with interference factor
        """
        ...
        # compute FLOPs, memory, and TP comm overhead
        ...
        return base_throughput
\n```
- For `workload_phase=aggregated`, the solver computes prefill and decode throughput separately, then combines them with an interference factor.  
\n```421:746:/mnt/projects/LLM_placement_solver/solver.py
def gpu_throughput_with_tp(..., phase: str = 'prefill', ...):
    if phase == 'aggregated':
        prefill_tp = ...
        decode_tp = ...
        aggregated_tp = ThroughputFunctions.calculate_aggregated_throughput(...)
        return aggregated_tp
...
def calculate_aggregated_throughput(..., interference_factor: float = 0.80) -> float:
    prefill_time = seq_len / prefill_throughput
    decode_time = output_len / decode_throughput
    base_throughput = (seq_len + output_len) / (prefill_time + decode_time)
    return base_throughput * interference_factor
\n```
- End-to-end throughput is the minimum of stage throughput and inter-stage network throughput, multiplied by pipeline efficiency (micro-batch bubbles) and a `real_world_efficiency` scalar from config. Cost per token is derived from per-instance hourly price and throughput.  
\n```1911:1974:/mnt/projects/LLM_placement_solver/solver.py
        stage_throughputs = [a['throughput'] for a in assignments]
        bottleneck_candidates = stage_throughputs + network_throughputs
        raw_throughput = min(bottleneck_candidates) if bottleneck_candidates else 0.0
        ...
        pipeline_efficiency = max(0.50, ideal_efficiency * (1.0 - scheduling_overhead))
        throughput_per_sec = raw_throughput * pipeline_efficiency * self.config.real_world_efficiency
        cost_per_hour = sum(self.gpu_types[gpu_type].cost_per_hour for gpu_type in used_instances)
        cost_per_token = cost_per_hour / (throughput_per_sec * 3600)
\n```

### What the `data.csv` Rows Represent
Each row is a solver-evaluated placement for a specific `(instance_family, TP, PP, input_length, output_length, batch_size)` setting. The important columns include:
- **Placement:** `placement`, `layer_mapping`, `tp_per_stage`, `pipeline_stages`, `num_gpus`
- **Performance:** `total_tokens_per_sec`, `input_tokens_per_sec`, `output_tokens_per_sec`
- **Cost:** `cost_per_hour`, `dollar_per_million_token`, `total_cost`
- **Hardware/cluster model:** `device_type`, `gpu_type`, `mem_per_gpu_gb`, `num_replicas`
- **Provenance:** `output_dir`, `source_path`, `status`

### Reliability Notes (From the Code)
- This is **not** a measured benchmark; it is an analytical roofline + communication model with heuristic efficiency factors (`real_world_efficiency`, batch efficiency, TP overhead, pipeline bubble model).
- Network costs come from the static bandwidth matrix in `config/network_bandwidth.csv`, not from measured cluster telemetry.
- Pricing is pulled from `config/cloud_instances_specs.csv` for the selected cloud provider, not from live cloud pricing APIs.

### Missing From This Dataset
The solver outputs do **not** capture:
- Actual serving engine/stack (vLLM/TGI/etc), kernel choices, or runtime profiling data.
- Real deployment/cluster logs, request traces, or observed latency distributions.
- The true physical network topology; only the configured bandwidth matrix is used.

If you need those details, they must be recorded outside this solver-based pipeline.

