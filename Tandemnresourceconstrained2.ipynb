{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NcE_CiAhNzGD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CmSvx3SPNzL4"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    name: str\n",
    "    device_type: str           # e.g. \"NVIDIA-L40S\"\n",
    "    device_count: int          # total GPUs on node\n",
    "    used_devices: int = 0      # GPUs currently allocated\n",
    "\n",
    "    @property\n",
    "    def free_devices(self) -> int:\n",
    "        return self.device_count - self.used_devices\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fabric:\n",
    "    name: str\n",
    "    nodes: List[Node]\n",
    "\n",
    "    def total_free_by_type(self) -> Dict[str, int]:\n",
    "        \"\"\"Return total free GPUs per device_type.\"\"\"\n",
    "        totals: Dict[str, int] = {}\n",
    "        for n in self.nodes:\n",
    "            totals.setdefault(n.device_type, 0)\n",
    "            totals[n.device_type] += n.free_devices\n",
    "        return totals\n",
    "\n",
    "    def alloc_on_type(\n",
    "        self, device_type: str, gpus_needed: int\n",
    "    ) -> Optional[List[Tuple[str, int]]]:\n",
    "        \"\"\"\n",
    "        Try to allocate gpus_needed GPUs of device_type across nodes.\n",
    "        Returns list of (node_name, num_gpus) or None if impossible.\n",
    "        Simple greedy: fill most-free nodes first.\n",
    "        \"\"\"\n",
    "        allocations: List[Tuple[str, int]] = []\n",
    "        remaining = gpus_needed\n",
    "\n",
    "        candidates = sorted(\n",
    "            [n for n in self.nodes if n.device_type == device_type],\n",
    "            key=lambda n: n.free_devices,\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        for node in candidates:\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            take = min(node.free_devices, remaining)\n",
    "            if take > 0:\n",
    "                allocations.append((node.name, take))\n",
    "                node.used_devices += take\n",
    "                remaining -= take\n",
    "\n",
    "        if remaining > 0:\n",
    "            # rollback\n",
    "            for name, num in allocations:\n",
    "                for n in self.nodes:\n",
    "                    if n.name == name:\n",
    "                        n.used_devices -= num\n",
    "                        break\n",
    "            return None\n",
    "\n",
    "        return allocations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8kwPlqGuNzPz"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JobSpec:\n",
    "    job_id: str\n",
    "    tenant_id: str\n",
    "    model_name: str\n",
    "    num_lines: int                # e.g. 4000, 2000\n",
    "    avg_input_tokens: int         # estimated per line\n",
    "    avg_output_tokens: int        # estimated per line\n",
    "    slo_hours: float              # SLO from now, in hours\n",
    "    job_type: str = \"batch\"       # \"batch\" or \"online\"\n",
    "    importance: int = 1           # 1-low, 3-high\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JobState:\n",
    "    spec: JobSpec\n",
    "    submitted_at: float\n",
    "    progress_frac: float = 0.0    # 0.0–1.0\n",
    "    device_type: Optional[str] = None\n",
    "    tp: int = 0\n",
    "    pp: int = 0\n",
    "    replicas: int = 0\n",
    "    allocated_gpus: int = 0\n",
    "    allocations: List[Tuple[str, int]] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def deadline_ts(self) -> float:\n",
    "        return self.submitted_at + self.spec.slo_hours * 3600.0\n",
    "\n",
    "    @property\n",
    "    def total_tokens(self) -> int:\n",
    "        return self.spec.num_lines * (self.spec.avg_input_tokens + self.spec.avg_output_tokens)\n",
    "\n",
    "    @property\n",
    "    def remaining_tokens(self) -> int:\n",
    "        return int((1.0 - self.progress_frac) * self.total_tokens)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerfEntry:\n",
    "    model_name: str\n",
    "    device_type: str\n",
    "    tp: int\n",
    "    pp: int\n",
    "    tokens_per_sec: float   # per replica (tp*pp GPUs)\n",
    "    mem_per_gpu_gb: float\n",
    "\n",
    "\n",
    "class PerfDB:\n",
    "    def __init__(self, entries: List[PerfEntry]):\n",
    "        self.entries = entries\n",
    "\n",
    "    def lookup(self, model_name: str, device_type: str, tp: int, pp: int) -> Optional[PerfEntry]:\n",
    "        for e in self.entries:\n",
    "            if (\n",
    "                e.model_name == model_name\n",
    "                and e.device_type == device_type\n",
    "                and e.tp == tp\n",
    "                and e.pp == pp\n",
    "            ):\n",
    "                return e\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iWNOeLVVNzSn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf DB head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>max_input_length</th>\n",
       "      <th>max_output_length</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>input_tokens_per_sec</th>\n",
       "      <th>output_tokens_per_sec</th>\n",
       "      <th>device_type</th>\n",
       "      <th>tp</th>\n",
       "      <th>pp</th>\n",
       "      <th>mem_per_gpu_gb</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>dollar_per_million_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>1186.94</td>\n",
       "      <td>950.39</td>\n",
       "      <td>236.55</td>\n",
       "      <td>g6e.48xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3998</td>\n",
       "      <td>3.124272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>165.60</td>\n",
       "      <td>132.59</td>\n",
       "      <td>33.00</td>\n",
       "      <td>g5.48xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5164</td>\n",
       "      <td>27.482886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>139.42</td>\n",
       "      <td>111.64</td>\n",
       "      <td>27.79</td>\n",
       "      <td>g5.48xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1766</td>\n",
       "      <td>32.642147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>139.13</td>\n",
       "      <td>111.40</td>\n",
       "      <td>27.73</td>\n",
       "      <td>g5.48xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1855</td>\n",
       "      <td>32.712023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>670.89</td>\n",
       "      <td>537.19</td>\n",
       "      <td>133.71</td>\n",
       "      <td>3x g5.12xlarge</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>5.087741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model_name  max_input_length  \\\n",
       "0  deepseek-ai/DeepSeek-R1-Distill-Llama-70B              2048   \n",
       "1  deepseek-ai/DeepSeek-R1-Distill-Llama-70B              2048   \n",
       "2  deepseek-ai/DeepSeek-R1-Distill-Llama-70B              2048   \n",
       "3  deepseek-ai/DeepSeek-R1-Distill-Llama-70B              2048   \n",
       "4  deepseek-ai/DeepSeek-R1-Distill-Llama-70B              2048   \n",
       "\n",
       "   max_output_length  tokens_per_sec  input_tokens_per_sec  \\\n",
       "0                512         1186.94                950.39   \n",
       "1                512          165.60                132.59   \n",
       "2                512          139.42                111.64   \n",
       "3                512          139.13                111.40   \n",
       "4                512          670.89                537.19   \n",
       "\n",
       "   output_tokens_per_sec     device_type  tp  pp  mem_per_gpu_gb  total_cost  \\\n",
       "0                 236.55    g6e.48xlarge   4   2             NaN      0.3998   \n",
       "1                  33.00     g5.48xlarge   4   2             NaN      3.5164   \n",
       "2                  27.79     g5.48xlarge   4   2             NaN      4.1766   \n",
       "3                  27.73     g5.48xlarge   4   2             NaN      4.1855   \n",
       "4                 133.71  3x g5.12xlarge   4   3             NaN      0.3906   \n",
       "\n",
       "   dollar_per_million_token  \n",
       "0                  3.124272  \n",
       "1                 27.482886  \n",
       "2                 32.642147  \n",
       "3                 32.712023  \n",
       "4                  5.087741  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device_type from perfdb: g6e.48xlarge\n"
     ]
    }
   ],
   "source": [
    "# Load your perfdb_l40s.csv\n",
    "# perf_df = pd.read_csv(\"perfdb_l40s.csv\")\n",
    "perf_df = pd.read_csv(\"gangmuk_perfdb.csv\")\n",
    "print(\"Perf DB head:\")\n",
    "display(perf_df.head())\n",
    "\n",
    "# Adjust these column names if your CSV headers differ.\n",
    "entries = [\n",
    "    PerfEntry(\n",
    "        model_name=row[\"model_name\"],\n",
    "        device_type=row[\"device_type\"],\n",
    "        tp=int(row[\"tp\"]),\n",
    "        pp=int(row[\"pp\"]),\n",
    "        tokens_per_sec=float(row[\"tokens_per_sec\"]),\n",
    "        mem_per_gpu_gb=float(row[\"mem_per_gpu_gb\"]),\n",
    "    )\n",
    "    for _, row in perf_df.iterrows()\n",
    "]\n",
    "\n",
    "perf_db = PerfDB(entries)\n",
    "\n",
    "# Deduce device type from DB\n",
    "default_device_type = perf_df[\"device_type\"].iloc[0]\n",
    "print(\"Default device_type from perfdb:\", default_device_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Xbn1ruBeNzU1"
   },
   "outputs": [],
   "source": [
    "def llm_choose_config_from_candidates(\n",
    "    job: JobState,\n",
    "    candidates: List[Dict[str, Any]],\n",
    "    model_id: str,\n",
    "    hf_token: str,\n",
    "    advisor_name: str,\n",
    "    top_k: int = 3,\n",
    "    temperature: float = 0.2,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Ask an HF LLM (Phi or other) to choose among top_k analytic candidates.\n",
    "    Returns the chosen config dict or None.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates_sorted = sorted(candidates, key=lambda c: c[\"gpu_time\"])\n",
    "    candidates_top = candidates_sorted[: min(top_k, len(candidates_sorted))]\n",
    "\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "    labeled = list(zip(labels, candidates_top))\n",
    "\n",
    "    client = InferenceClient(model=model_id, token=hf_token)\n",
    "\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(\n",
    "        f\"You are an expert GPU scheduler ({advisor_name}) choosing tensor/pipeline \"\n",
    "        \"parallelism for an LLM job.\\n\"\n",
    "    )\n",
    "    prompt_lines.append(\"Goals, in order:\\n\")\n",
    "    prompt_lines.append(\"1. The job must finish within its SLO (deadline).\\n\")\n",
    "    prompt_lines.append(\"2. Minimize total GPU-hours used.\\n\")\n",
    "    prompt_lines.append(\"3. Prefer simpler configs (fewer TP/PP/replicas) when close.\\n\\n\")\n",
    "\n",
    "    prompt_lines.append(\"Job:\\n\")\n",
    "    prompt_lines.append(f\"- Job ID: {job.spec.job_id}\\n\")\n",
    "    prompt_lines.append(f\"- Model: {job.spec.model_name}\\n\")\n",
    "    prompt_lines.append(f\"- Lines (requests): {job.spec.num_lines}\\n\")\n",
    "    prompt_lines.append(f\"- Avg input tokens: {job.spec.avg_input_tokens}\\n\")\n",
    "    prompt_lines.append(f\"- Avg output tokens: {job.spec.avg_output_tokens}\\n\")\n",
    "    prompt_lines.append(f\"- SLO: {job.spec.slo_hours} hours\\n\")\n",
    "    prompt_lines.append(f\"- Total tokens (approx): {job.total_tokens}\\n\\n\")\n",
    "\n",
    "    prompt_lines.append(\"Candidate configs:\\n\")\n",
    "    for label, cfg in labeled:\n",
    "        prompt_lines.append(\n",
    "            f\"Plan {label}:\\n\"\n",
    "            f\"- tp: {cfg['tp']}\\n\"\n",
    "            f\"- pp: {cfg['pp']}\\n\"\n",
    "            f\"- replicas: {cfg['replicas']}\\n\"\n",
    "            f\"- total GPUs: {cfg['gpus_needed']}\\n\"\n",
    "            f\"- predicted runtime: {cfg['runtime_hours']:.2f} hours\\n\"\n",
    "            f\"- GPU-hours: {cfg['gpu_time']:.2f}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt_lines.append(\n",
    "        \"Which plan best satisfies the goals? Respond with exactly one line:\\n\"\n",
    "        \"Best plan: A\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = \"\".join(prompt_lines)\n",
    "\n",
    "    try:\n",
    "        resp = client.text_generation(\n",
    "            prompt,\n",
    "            max_new_tokens=64,\n",
    "            temperature=temperature,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        text = resp.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM {advisor_name}] Error calling HF model: {e}\")\n",
    "        return None\n",
    "\n",
    "    chosen_label = None\n",
    "    for label, _ in labeled:\n",
    "        if f\"Best plan: {label}\" in text or f\"best plan: {label}\" in text:\n",
    "            chosen_label = label\n",
    "            break\n",
    "        if f\"Plan {label}\" in text or f\"plan {label}\" in text:\n",
    "            chosen_label = label\n",
    "            break\n",
    "\n",
    "    if chosen_label is None:\n",
    "        # fallback\n",
    "        for label, _ in labeled:\n",
    "            if f\" {label}\" in text:\n",
    "                chosen_label = label\n",
    "                break\n",
    "\n",
    "    if chosen_label is None:\n",
    "        print(f\"[LLM {advisor_name}] Could not parse choice from: {text}\")\n",
    "        return None\n",
    "\n",
    "    for label, cfg in labeled:\n",
    "        if label == chosen_label:\n",
    "            return cfg\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pv61vZ17NzX5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/venv/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2263: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_c_pmi_model(model_name: str = \"microsoft/DialoGPT-large\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def _avg_nll(text: str, tokenizer, model) -> float:\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs[0]\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def c_pmi_score(\n",
    "    context: str,\n",
    "    hypothesis: str,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    sep: str = \" <|endoftext|> \",\n",
    ") -> float:\n",
    "    lpx = -_avg_nll(context + sep + hypothesis, tokenizer, model)\n",
    "    lpx_context = -_avg_nll(context, tokenizer, model)\n",
    "    lpx_hyp = -_avg_nll(hypothesis, tokenizer, model)\n",
    "    pmi = lpx - lpx_context - lpx_hyp\n",
    "    return pmi\n",
    "\n",
    "\n",
    "def c_pmi_rank_plans(\n",
    "    context: str,\n",
    "    plan_labels,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    temperature: float = 1.0,\n",
    "):\n",
    "    hypotheses = [\n",
    "        f\"In this situation, the best plan is {label}.\"\n",
    "        for label in plan_labels\n",
    "    ]\n",
    "    scores = [\n",
    "        c_pmi_score(context, hyp, tokenizer, model)\n",
    "        for hyp in hypotheses\n",
    "    ]\n",
    "    max_s = max(scores)\n",
    "    exps = [math.exp((s - max_s) / max(temperature, 1e-6)) for s in scores]\n",
    "    Z = sum(exps)\n",
    "    probs = [e / Z for e in exps]\n",
    "    label_to_prob = {label: prob for label, prob in zip(plan_labels, probs)}\n",
    "    best_label = max(label_to_prob.items(), key=lambda x: x[1])[0]\n",
    "    return best_label, label_to_prob\n",
    "\n",
    "\n",
    "# Load the C-PMI scorer model\n",
    "c_pmi_model, c_pmi_tokenizer = load_c_pmi_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vJxTS335Nzaf"
   },
   "outputs": [],
   "source": [
    "class OrcaOrchestrator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fabric: Fabric,\n",
    "        perf_db: PerfDB,\n",
    "        hf_token: Optional[str] = None,\n",
    "        hf_phi_model_id: Optional[str] = None,\n",
    "        hf_other_model_id: Optional[str] = None,\n",
    "    ):\n",
    "        self.fabric = fabric\n",
    "        self.perf_db = perf_db\n",
    "        self.jobs: Dict[str, JobState] = {}\n",
    "\n",
    "        self.hf_token = hf_token\n",
    "        self.hf_phi_model_id = hf_phi_model_id\n",
    "        self.hf_other_model_id = hf_other_model_id\n",
    "\n",
    "    def submit_job(self, spec: JobSpec) -> JobState:\n",
    "        now = time.time()\n",
    "        job_state = JobState(spec=spec, submitted_at=now)\n",
    "        self.jobs[spec.job_id] = job_state\n",
    "        self._schedule_new_job(job_state, now)\n",
    "        return job_state\n",
    "\n",
    "    def _schedule_new_job(self, job: JobState, now: float):\n",
    "        totals = self.fabric.total_free_by_type()\n",
    "        if not totals:\n",
    "            print(f\"[Orca] No free GPUs for job {job.spec.job_id}\")\n",
    "            return\n",
    "\n",
    "        device_type = next(iter(totals.keys()))\n",
    "        free_gpus = totals[device_type]\n",
    "\n",
    "        candidates = self._enumerate_configs(job, device_type, free_gpus, now)\n",
    "        if not candidates:\n",
    "            print(f\"[Orca] Cannot meet SLO for job {job.spec.job_id} with free GPUs\")\n",
    "            return\n",
    "\n",
    "        # --- 1) Math advisor: analytic best (minimal GPU-hours) ---\n",
    "        math_cfg = min(candidates, key=lambda c: c[\"gpu_time\"])\n",
    "\n",
    "        # --- 2) Phi advisor ---\n",
    "        phi_cfg = None\n",
    "        if self.hf_token and self.hf_phi_model_id and len(candidates) > 1:\n",
    "            phi_cfg = llm_choose_config_from_candidates(\n",
    "                job=job,\n",
    "                candidates=candidates,\n",
    "                model_id=self.hf_phi_model_id,\n",
    "                hf_token=self.hf_token,\n",
    "                advisor_name=\"PhiAdvisor\",\n",
    "                top_k=3,\n",
    "            )\n",
    "\n",
    "        # --- 3) Other reasoning advisor ---\n",
    "        other_cfg = None\n",
    "        if self.hf_token and self.hf_other_model_id and len(candidates) > 1:\n",
    "            other_cfg = llm_choose_config_from_candidates(\n",
    "                job=job,\n",
    "                candidates=candidates,\n",
    "                model_id=self.hf_other_model_id,\n",
    "                hf_token=self.hf_token,\n",
    "                advisor_name=\"OtherAdvisor\",\n",
    "                top_k=3,\n",
    "            )\n",
    "\n",
    "        # --- Build context string for C-PMI judge ---\n",
    "        plans = [(\"Math\", math_cfg)]\n",
    "        if phi_cfg is not None:\n",
    "            plans.append((\"PhiAdvisor\", phi_cfg))\n",
    "        if other_cfg is not None:\n",
    "            plans.append((\"OtherAdvisor\", other_cfg))\n",
    "\n",
    "        context_lines = []\n",
    "        context_lines.append(\n",
    "            \"We are choosing a GPU parallelism configuration for an LLM job.\\n\"\n",
    "        )\n",
    "        context_lines.append(\"Goals:\\n\")\n",
    "        context_lines.append(\"1. Meet the SLO (deadline).\\n\")\n",
    "        context_lines.append(\"2. Minimize total GPU-hours.\\n\")\n",
    "        context_lines.append(\"3. Prefer simpler configs when close.\\n\\n\")\n",
    "\n",
    "        context_lines.append(\"Job:\\n\")\n",
    "        context_lines.append(f\"- Model: {job.spec.model_name}\\n\")\n",
    "        context_lines.append(f\"- Lines: {job.spec.num_lines}\\n\")\n",
    "        context_lines.append(f\"- Avg input tokens: {job.spec.avg_input_tokens}\\n\")\n",
    "        context_lines.append(f\"- Avg output tokens: {job.spec.avg_output_tokens}\\n\")\n",
    "        context_lines.append(f\"- SLO: {job.spec.slo_hours} hours\\n\")\n",
    "        context_lines.append(f\"- Total tokens: {job.total_tokens}\\n\\n\")\n",
    "\n",
    "        context_lines.append(\"Advisor proposals:\\n\")\n",
    "        for name, cfg in plans:\n",
    "            context_lines.append(\n",
    "                f\"{name} proposes:\\n\"\n",
    "                f\"  - tp: {cfg['tp']}\\n\"\n",
    "                f\"  - pp: {cfg['pp']}\\n\"\n",
    "                f\"  - replicas: {cfg['replicas']}\\n\"\n",
    "                f\"  - total GPUs: {cfg['gpus_needed']}\\n\"\n",
    "                f\"  - predicted runtime: {cfg['runtime_hours']:.2f} hours\\n\"\n",
    "                f\"  - GPU-hours: {cfg['gpu_time']:.2f}\\n\\n\"\n",
    "            )\n",
    "\n",
    "        context_str = \"\".join(context_lines)\n",
    "        plan_labels = [name for (name, _) in plans]\n",
    "\n",
    "        # --- 4) C-PMI judge decides which advisor is most likely \"correct\" ---\n",
    "        best_label, probs = c_pmi_rank_plans(\n",
    "            context=context_str,\n",
    "            plan_labels=plan_labels,\n",
    "            tokenizer=c_pmi_tokenizer,\n",
    "            model=c_pmi_model,\n",
    "        )\n",
    "\n",
    "        print(\"[C-PMI Judge] Probabilities:\", probs)\n",
    "        print(\"[C-PMI Judge] Chose:\", best_label)\n",
    "\n",
    "        chosen_cfg = math_cfg\n",
    "        for name, cfg in plans:\n",
    "            if name == best_label:\n",
    "                chosen_cfg = cfg\n",
    "                break\n",
    "\n",
    "        # --- 5) Place chosen config on the fabric ---\n",
    "        alloc = self.fabric.alloc_on_type(device_type, chosen_cfg[\"gpus_needed\"])\n",
    "        if alloc is None:\n",
    "            print(f\"[Orca] Placement failed for job {job.spec.job_id} (fragmentation)\")\n",
    "            return\n",
    "\n",
    "        job.device_type = device_type\n",
    "        job.tp = chosen_cfg[\"tp\"]\n",
    "        job.pp = chosen_cfg[\"pp\"]\n",
    "        job.replicas = chosen_cfg[\"replicas\"]\n",
    "        job.allocated_gpus = chosen_cfg[\"gpus_needed\"]\n",
    "        job.allocations = alloc\n",
    "\n",
    "        print(\n",
    "            f\"[Orca] Scheduled {job.spec.job_id} on {device_type}: \"\n",
    "            f\"{chosen_cfg['gpus_needed']} GPUs (tp={chosen_cfg['tp']}, pp={chosen_cfg['pp']}, \"\n",
    "            f\"replicas={chosen_cfg['replicas']}), predicted_runtime={chosen_cfg['runtime_hours']:.2f}h\"\n",
    "        )\n",
    "\n",
    "    def _enumerate_configs(\n",
    "        self,\n",
    "        job: JobState,\n",
    "        device_type: str,\n",
    "        free_gpus_of_type: int,\n",
    "        now: float,\n",
    "        guard_frac: float = 0.1,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        total_tokens = job.remaining_tokens\n",
    "        T_left = job.deadline_ts - now\n",
    "        if T_left <= 0:\n",
    "            return []\n",
    "\n",
    "        effective_horizon = T_left * (1.0 - guard_frac)\n",
    "        if effective_horizon <= 0:\n",
    "            return []\n",
    "\n",
    "        configs: List[Dict[str, Any]] = []\n",
    "\n",
    "        # You can expand TP/PP candidate sets later\n",
    "        candidate_tps = [1, 2, 4]\n",
    "        candidate_pps = [1]  #change/add em\n",
    "\n",
    "        for tp in candidate_tps:\n",
    "            for pp in candidate_pps:\n",
    "                gpus_per_replica = tp * pp\n",
    "                if gpus_per_replica > free_gpus_of_type:\n",
    "                    continue\n",
    "\n",
    "                pe = self.perf_db.lookup(job.spec.model_name, device_type, tp, pp)\n",
    "                if pe is None:\n",
    "                    continue\n",
    "\n",
    "                # basic memory check; adjust if needed\n",
    "                # here we assume device has at least pe.mem_per_gpu_gb\n",
    "                max_replicas = free_gpus_of_type // gpus_per_replica\n",
    "                if max_replicas == 0:\n",
    "                    continue\n",
    "\n",
    "                for replicas in range(1, max_replicas + 1):\n",
    "                    tokens_per_sec_total = replicas * pe.tokens_per_sec\n",
    "                    runtime_sec = total_tokens / tokens_per_sec_total\n",
    "\n",
    "                    if runtime_sec <= effective_horizon:\n",
    "                        gpu_count = replicas * gpus_per_replica\n",
    "                        gpu_time = gpu_count * runtime_sec / 3600.0\n",
    "\n",
    "                        configs.append({\n",
    "                            \"tp\": tp,\n",
    "                            \"pp\": pp,\n",
    "                            \"replicas\": replicas,\n",
    "                            \"gpus_needed\": gpu_count,\n",
    "                            \"runtime_hours\": runtime_sec / 3600.0,\n",
    "                            \"gpu_time\": gpu_time,\n",
    "                        })\n",
    "                        break  # minimal replicas that satisfy SLO for this tp/pp\n",
    "\n",
    "        return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dTfE3ExkNzdZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Orca] Cannot meet SLO for job job-llama-70b-batch with free GPUs\n",
      "[Orca] Cannot meet SLO for job job-deepseek-70b-batch with free GPUs\n",
      "\n",
      "--- Job placements ---\n",
      "JobState(spec=JobSpec(job_id='job-llama-70b-batch', tenant_id='tenant-A', model_name='llama-3.3-70b', num_lines=4000, avg_input_tokens=2048, avg_output_tokens=32, slo_hours=12.0, job_type='batch', importance=2), submitted_at=1769235194.1265135, progress_frac=0.0, device_type=None, tp=0, pp=0, replicas=0, allocated_gpus=0, allocations=[])\n",
      "JobState(spec=JobSpec(job_id='job-deepseek-70b-batch', tenant_id='tenant-A', model_name='deepseek-distill-70b', num_lines=2000, avg_input_tokens=1024, avg_output_tokens=1024, slo_hours=6.0, job_type='batch', importance=2), submitted_at=1769235194.1268284, progress_frac=0.0, device_type=None, tp=0, pp=0, replicas=0, allocated_gpus=0, allocations=[])\n",
      "\n",
      "--- Fabric usage ---\n",
      "node-0 used/free: 0 / 4\n",
      "node-1 used/free: 0 / 4\n",
      "node-2 used/free: 0 / 4\n",
      "node-3 used/free: 0 / 4\n"
     ]
    }
   ],
   "source": [
    "# ----- Build a small L40S fabric -----\n",
    "# We’ll use 4 nodes * 4 GPUs each for demo (16 GPUs total)\n",
    "nodes = [\n",
    "    Node(name=f\"node-{i}\", device_type=default_device_type, device_count=4)\n",
    "    for i in range(4)\n",
    "]\n",
    "fabric = Fabric(name=\"l40s-fabric\", nodes=nodes)\n",
    "\n",
    "# ----- HF models for advisors -----\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "HF_MODEL_PHI   = \"microsoft/Phi-3-mini-4k-instruct\"   # advisor 1\n",
    "HF_MODEL_OTHER = \"Qwen/Qwen2.5-1.5B-Instruct\"         # advisor 2 (example)\n",
    "\n",
    "orca = OrcaOrchestrator(\n",
    "    fabric=fabric,\n",
    "    perf_db=perf_db,\n",
    "    hf_token=HF_TOKEN,\n",
    "    hf_phi_model_id=HF_MODEL_PHI,\n",
    "    hf_other_model_id=HF_MODEL_OTHER,\n",
    ")\n",
    "\n",
    "# ----- Example jobs -----\n",
    "\n",
    "# Job 1: LLaMA 70B-like batch classification job, 12h SLO\n",
    "job1_spec = JobSpec(\n",
    "    job_id=\"job-llama-70b-batch\",\n",
    "    tenant_id=\"tenant-A\",\n",
    "    model_name=\"llama-3.3-70b\",   # must match something in perfdb_l40s.csv\n",
    "    num_lines=4000,\n",
    "    avg_input_tokens=2048,\n",
    "    avg_output_tokens=32,\n",
    "    slo_hours=12.0,\n",
    "    job_type=\"batch\",\n",
    "    importance=2,\n",
    ")\n",
    "\n",
    "# Job 2: DeepSeek distill 70B translation job, 6h SLO\n",
    "job2_spec = JobSpec(\n",
    "    job_id=\"job-deepseek-70b-batch\",\n",
    "    tenant_id=\"tenant-A\",\n",
    "    model_name=\"deepseek-distill-70b\",  # must match perfdb\n",
    "    num_lines=2000,\n",
    "    avg_input_tokens=1024,\n",
    "    avg_output_tokens=1024,\n",
    "    slo_hours=6.0,\n",
    "    job_type=\"batch\",\n",
    "    importance=2,\n",
    ")\n",
    "\n",
    "job1_state = orca.submit_job(job1_spec)\n",
    "job2_state = orca.submit_job(job2_spec)\n",
    "\n",
    "print(\"\\n--- Job placements ---\")\n",
    "print(job1_state)\n",
    "print(job2_state)\n",
    "\n",
    "print(\"\\n--- Fabric usage ---\")\n",
    "for n in fabric.nodes:\n",
    "    print(n.name, \"used/free:\", n.used_devices, \"/\", n.device_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
