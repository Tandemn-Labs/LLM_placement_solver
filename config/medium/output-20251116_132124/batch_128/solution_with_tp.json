{
  "config": {
    "model_name": "llama-8b",
    "num_decoder_layers": 32,
    "sequence_length": 4096,
    "min_batch_size": 128,
    "max_batch_size": 128,
    "optimal_batch_size": 128,
    "d_model": 8192,
    "d_hidden": 22016,
    "max_pipeline_stages": 8,
    "min_memory_utilization": 0.5
  },
  "solution": {
    "objective_value": 12583.925755692748,
    "batch_size": 128,
    "throughput_tokens_per_sec": 3775.177726707824,
    "cost_per_hour": 16.385,
    "cost_per_token": 1.2056091708450417e-06,
    "meets_cost_threshold": false,
    "tp_configuration": {
      "A100": 8,
      "L40": 8,
      "A10": 8,
      "V100": 8
    },
    "gpu_assignments": [
      {
        "gpu_type": "A100",
        "partition_id": 12,
        "gpu_ids": [
          0,
          1,
          2,
          3
        ],
        "global_gpu_ids": [
          0,
          1,
          2,
          3
        ],
        "tp_degree": 4,
        "start_layer": 1,
        "end_layer": 32,
        "segment_size": 32,
        "throughput": 12583.925755692744
      }
    ],
    "network_connections": [],
    "solve_status": 2,
    "num_pipeline_stages": 1
  }
}