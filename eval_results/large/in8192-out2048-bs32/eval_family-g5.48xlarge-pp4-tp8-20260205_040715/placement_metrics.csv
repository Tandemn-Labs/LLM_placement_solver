model_name,max_input_length,max_output_length,batch_size,total_tokens_per_sec,input_tokens_per_sec,output_tokens_per_sec,cost_per_hour,dollar_per_million_token,total_runtime_hours,total_cost,pipeline_stages,device_type,pp,mem_per_gpu_gb,placement,layer_mapping,tp_per_stage,num_gpus,total_layers,status
llama3-70b,8192,2048,32,411.75,329.40,82.35,65.16,43.958192,0.67,43.96,4,"g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",4,"{PP_1:24.0, PP_2:24.0, PP_3:24.0, PP_4:24.0}","{PP_1:{g5.48xlarge#0:8}, PP_2:{g5.48xlarge#1:8}, PP_3:{g5.48xlarge#2:8}, PP_4:{g5.48xlarge#3:8}}","{PP_1:1-20, PP_2:21-40, PP_3:41-60, PP_4:61-80}","{PP_1:8, PP_2:8, PP_3:8, PP_4:8}",32,80,SUCCESS
