model_name,max_input_length,max_output_length,batch_size,total_tokens_per_sec,input_tokens_per_sec,output_tokens_per_sec,cost_per_hour,dollar_per_million_token,total_runtime_hours,total_cost,pipeline_stages,device_type,pp,mem_per_gpu_gb,placement,layer_mapping,tp_per_stage,num_gpus,total_layers,status
llama3-70b,8192,2048,32,282.07,225.66,56.41,124.84,122.938261,0.98,122.94,4,"p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",4,"{PP_1:32.0, PP_2:32.0, PP_3:32.0, PP_4:32.0}","{PP_1:{p3dn.24xlarge#0:4}, PP_2:{p3dn.24xlarge#1:4}, PP_3:{p3dn.24xlarge#2:4}, PP_4:{p3dn.24xlarge#3:4}}","{PP_1:1-20, PP_2:21-40, PP_3:41-60, PP_4:61-80}","{PP_1:4, PP_2:4, PP_3:4, PP_4:4}",16,80,SUCCESS
