model_name,max_input_length,max_output_length,batch_size,total_tokens_per_sec,input_tokens_per_sec,output_tokens_per_sec,cost_per_hour,dollar_per_million_token,total_runtime_hours,total_cost,pipeline_stages,device_type,pp,mem_per_gpu_gb,placement,layer_mapping,tp_per_stage,num_gpus,total_layers,status
llama3-70b,2048,512,32,208.17,166.54,41.63,11.34,15.131786,1.33,15.13,2,"g5.12xlarge#0,g5.12xlarge#1",2,"{PP_1:24.0, PP_2:24.0}","{PP_1:{g5.12xlarge#0:4}, PP_2:{g5.12xlarge#1:4}}","{PP_1:1-40, PP_2:41-80}","{PP_1:4, PP_2:4}",8,80,SUCCESS
