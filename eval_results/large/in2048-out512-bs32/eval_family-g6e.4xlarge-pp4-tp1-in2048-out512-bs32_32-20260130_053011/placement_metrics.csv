model_name,max_input_length,max_output_length,batch_size,total_tokens_per_sec,input_tokens_per_sec,output_tokens_per_sec,cost_per_hour,dollar_per_million_token,total_runtime_hours,total_cost,pipeline_stages,device_type,pp,mem_per_gpu_gb,placement,layer_mapping,tp_per_stage,num_gpus,total_layers,status
llama3-70b,2048,512,32,193.51,154.81,38.70,13.68,19.636828,1.44,19.64,4,"g6e.4xlarge#0,g6e.4xlarge#1,g6e.4xlarge#2,g6e.4xlarge#3",4,"{PP_1:48.0, PP_2:48.0, PP_3:48.0, PP_4:48.0}","{PP_1:{g6e.4xlarge#0:1}, PP_2:{g6e.4xlarge#1:1}, PP_3:{g6e.4xlarge#2:1}, PP_4:{g6e.4xlarge#3:1}}","{PP_1:1-20, PP_2:21-40, PP_3:41-60, PP_4:61-80}","{PP_1:1, PP_2:1, PP_3:1, PP_4:1}",4,80,SUCCESS
