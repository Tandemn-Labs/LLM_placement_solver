batch_size,cost_per_hour,cost_per_token,device_type,dollar_per_million_token,error,gpu_types,input_length,instance_family,max_batch_size,min_batch_size,min_network_throughput,model_name,num_decoder_layers,num_gpus,num_instances,num_pipeline_stages,output_length,pipeline_efficiency,pp_stages,raw_throughput_tokens_per_sec,real_world_efficiency,result_dir,sequence_length,status,throughput_per_dollar,throughput_per_gpu,throughput_tokens_per_sec,total_runtime_hours,tp_degree,tp_degrees,workload_phase
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.48xlarge#0,2048,g5.48xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,27.39,2.4428250031496847e-05,NVIDIA L40S,24.428250031496848,,g6e.48xlarge#0,2048,g6e.48xlarge,32,32,,llama3-70b,80,4,1,1,512,1.0,1,1038.1877980785146,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,11.371169748943204,77.86408485588859,311.45633942355437,0.8918674710294576,4,4,
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.12xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.12xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,32,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.48xlarge#0, TP=8, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89 | Stage 2 (g6e.48xlarge#1, TP=8, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89 | Stage 3 (g6e.48xlarge#2, TP=8, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89 | Stage 4 (g6e.48xlarge#3, TP=8, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89","g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,32,32,,llama3-70b,80,32,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,8,8,aggregated
,,,NVIDIA A100,,,,2048,p4de.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.12xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.12xlarge#0,2048,g6e.12xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.48xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.48xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA L40S,,,,2048,g6e.4xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.4xlarge#0,2048,g5.4xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.48xlarge#0, TP=8, layers 1-40, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89 | Stage 2 (g6e.48xlarge#1, TP=8, layers 41-80, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 80 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((24.00 - 1.15) / 0.26) = 89","g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,32,32,,llama3-70b,80,16,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,8,8,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.24xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 2 (g5.24xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 3 (g5.24xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 4 (g5.24xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21","g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,54.8,7.866214868404545e-05,NVIDIA L40S,78.66214868404545,,"g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,32,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,3.5312762545236414,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.4xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.4xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.4xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.4xlarge#0,g5.4xlarge#1,g5.4xlarge#2,g5.4xlarge#3",2048,g5.4xlarge,32,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,13.7,1.221858435310357e-05,NVIDIA L40S,12.21858435310357,,g6e.24xlarge#0,2048,g6e.24xlarge,32,32,,llama3-70b,80,4,1,1,512,1.0,1,1038.1877980785146,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,22.73403937398207,77.86408485588859,311.45633942355437,0.8918674710294576,4,4,
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (p3dn.24xlarge#0, TP=8, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58 | Stage 2 (p3dn.24xlarge#1, TP=8, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58 | Stage 3 (p3dn.24xlarge#2, TP=8, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58 | Stage 4 (p3dn.24xlarge#3, TP=8, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58","p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,32,32,,llama3-70b,80,32,4,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,8,8,aggregated
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.12xlarge#0,2048,g5.12xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.48xlarge#0,2048,g5.48xlarge,32,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.48xlarge#0, TP=8, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43 | Stage 2 (g5.48xlarge#1, TP=8, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43 | Stage 3 (g5.48xlarge#2, TP=8, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43 | Stage 4 (g5.48xlarge#3, TP=8, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43","g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,32,32,,llama3-70b,80,32,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,8,8,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
32,22.68,3.9163206222846e-05,NVIDIA A10G,39.163206222846,,"g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,32,32,102400.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,7.09282524513366,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,32.58,4.347386249852155e-05,NVIDIA A10G,43.473862498521555,,"g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,32,32,204800.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,6.389535270467959,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,124.84,0.0001789451970545954,NVIDIA V100,178.94519705459538,,"p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,32,32,102400.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1256.0458631198826,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,1.5523064175510057,24.223741645883447,193.78993316706757,1.433396323731139,2,2,
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,13.68,1.9636828357623024e-05,NVIDIA L40S,19.636828357623024,,"g6e.4xlarge#0,g6e.4xlarge#1,g6e.4xlarge#2,g6e.4xlarge#3",2048,g6e.4xlarge,32,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,14.145755756425112,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.12xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.12xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.12xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,32,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA A10G,,,,2048,g5.4xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.12xlarge#0, TP=2, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 2 (g6e.12xlarge#1, TP=2, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 3 (g6e.12xlarge#2, TP=2, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 4 (g6e.12xlarge#3, TP=2, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21","g6e.12xlarge#0,g6e.12xlarge#1,g6e.12xlarge#2,g6e.12xlarge#3",2048,g6e.12xlarge,32,32,,llama3-70b,80,8,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.12xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.12xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.12xlarge#0,g6e.12xlarge#1",2048,g6e.12xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.24xlarge#0,2048,g6e.24xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.48xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 2 (g5.48xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 3 (g5.48xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 4 (g5.48xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21","g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.48xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.48xlarge#0,2048,g6e.48xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.12xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.48xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 2 (g6e.48xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 3 (g6e.48xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 4 (g6e.48xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44","g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.24xlarge#0,2048,g5.24xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053010,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.48xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.48xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.48xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,32,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,65.16,0.00011251651311643056,NVIDIA A10G,112.51651311643056,,"g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,32,32,204800.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053010,2048,SUCCESS,2.4687734278642024,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.24xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 2 (g6e.24xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 3 (g6e.24xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 4 (g6e.24xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44","g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (p3dn.24xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (p3dn.24xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,32,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.12xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 2 (g5.12xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 3 (g5.12xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21 | Stage 4 (g5.12xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((12.00 - 1.51) / 0.52) = 21","g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,16.28,2.1723587522281492e-05,NVIDIA A10G,21.72358752228149,,"g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,32,32,204800.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,12.786920092865238,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (p3dn.24xlarge#0, TP=8, layers 1-40, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58 | Stage 2 (p3dn.24xlarge#1, TP=8, layers 41-80, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 58 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((16.00 - 1.15) / 0.26) = 58","p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,32,32,,llama3-70b,80,16,2,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,8,8,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.12xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.12xlarge#0,2048,g6e.12xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.24xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.24xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.24xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,32,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p4de.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=80.0GB. Max feasible layers=74 (max_required=79.33GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",p4de.24xlarge#0,2048,p4de.24xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.48xlarge#0, TP=8, layers 1-40, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43 | Stage 2 (g5.48xlarge#1, TP=8, layers 41-80, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 43 layers. Target memory=12.00GB (= 50% of 24.0GB), activation=1.15GB, per_layer=0.26GB ⇒ required_layers=ceil((12.00 - 1.15) / 0.26) = 43","g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,32,32,,llama3-70b,80,16,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,8,8,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,27.4,3.9331074342022725e-05,NVIDIA L40S,39.331074342022724,,"g6e.12xlarge#0,g6e.12xlarge#1,g6e.12xlarge#2,g6e.12xlarge#3",2048,g6e.12xlarge,32,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,7.062552509047283,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.48xlarge#0, TP=2, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 2 (g6e.48xlarge#1, TP=2, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 3 (g6e.48xlarge#2, TP=2, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 4 (g6e.48xlarge#3, TP=2, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21","g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,32,32,,llama3-70b,80,8,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.12xlarge#0,2048,g5.12xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA A100,,,,2048,p4de.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=32.0GB. Max feasible layers=59 (max_required=31.93GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,32,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.48xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.48xlarge#0,2048,g6e.48xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,13.7,1.524017869908187e-05,NVIDIA L40S,15.24017869908187,,"g6e.12xlarge#0,g6e.12xlarge#1",2048,g6e.12xlarge,32,32,819200.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,18.22667458581127,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.24xlarge#0,2048,g5.24xlarge,32,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053010,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.48xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.48xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,32,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.48xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053010,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.4xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.4xlarge#0,g5.4xlarge#1",2048,g5.4xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,40.96,9.39992626541533e-06,NVIDIA A100,9.39992626541533,,p4de.24xlarge#0,2048,p4de.24xlarge,32,32,,llama3-70b,80,8,1,1,512,1.0,1,4034.7046194888626,0.3,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,29.55105922477194,151.30142323083234,1210.4113858466587,0.22949038733924146,8,8,
,,,NVIDIA L40S,,,,2048,g6e.4xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.24xlarge#0,2048,g5.24xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,27.39,1.4183560578532578e-05,NVIDIA L40S,14.183560578532578,,g6e.48xlarge#0,2048,g6e.48xlarge,32,32,,llama3-70b,80,8,1,1,512,1.0,1,1788.0637919293856,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,19.584488411055702,67.05239219735196,536.4191375788157,0.5178371879712514,8,8,
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.4xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.48xlarge#0,2048,g5.48xlarge,32,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (p3dn.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,109.56,0.00015726687974131423,NVIDIA L40S,157.26687974131423,,"g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,32,32,409600.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,1.766282756004888,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp8-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,8,,aggregated
,,,NVIDIA A10G,,,,2048,g5.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (p3dn.24xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (p3dn.24xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (p3dn.24xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,32,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g6e.24xlarge#0, TP=2, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 2 (g6e.24xlarge#1, TP=2, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 3 (g6e.24xlarge#2, TP=2, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21 | Stage 4 (g6e.24xlarge#3, TP=2, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 21 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=3.02GB, per_layer=1.03GB ⇒ required_layers=ceil((24.00 - 3.02) / 1.03) = 21","g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,32,32,,llama3-70b,80,8,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p4de.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=80.0GB. Max feasible layers=35 (max_required=78.22GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",p4de.24xlarge#0,2048,p4de.24xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
,,,NVIDIA L40S,,,,2048,g6e.24xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.4xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.12xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (p3dn.24xlarge#0, TP=4, layers 1-20, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 29 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((16.00 - 1.51) / 0.52) = 29 | Stage 2 (p3dn.24xlarge#1, TP=4, layers 21-40, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 29 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((16.00 - 1.51) / 0.52) = 29 | Stage 3 (p3dn.24xlarge#2, TP=4, layers 41-60, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 29 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((16.00 - 1.51) / 0.52) = 29 | Stage 4 (p3dn.24xlarge#3, TP=4, layers 61-80, size=20): underutilized GPU: segment uses 20 layers, but the minimum to reach 50% of GPU memory is 29 layers. Target memory=16.00GB (= 50% of 32.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((16.00 - 1.51) / 0.52) = 29","p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,32,32,,llama3-70b,80,16,4,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,4,4,aggregated
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A100,,,,2048,p4de.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
,,,NVIDIA A10G,,,,2048,g5.4xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp4-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A100,,,,2048,p4de.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
,,,NVIDIA L40S,,,,2048,g6e.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp4-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,4,,aggregated
32,31.21,2.596816946851195e-05,NVIDIA V100,25.96816946851195,,p3dn.24xlarge#0,2048,p3dn.24xlarge,32,32,,llama3-70b,80,8,1,1,512,1.0,1,1112.829619476605,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,10.696856323068937,41.73111073037269,333.84888584298153,0.832046442438704,8,8,
32,16.29,1.73201775368366e-05,NVIDIA A10G,17.320177536836603,,g5.48xlarge#0,2048,g5.48xlarge,32,32,,llama3-70b,80,8,1,1,512,1.0,1,870.8532751037949,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp8-in2048-out512-bs32_32-20260130_053010,2048,SUCCESS,16.03781353782311,32.656997816392305,261.25598253113844,1.0632398733478576,8,8,
,,,NVIDIA V100,,,,2048,p3dn.24xlarge,32,32,,,,,,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,1,,aggregated
,,,NVIDIA A10G,,,,2048,g5.12xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,62.42,6.769193747937186e-05,NVIDIA V100,67.69193747937186,,"p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,32,32,102400.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,1185.8522112669311,0.29999999999999993,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,4.103557796117544,32.01800970420714,256.1440776336571,1.0844591073273286,4,4,
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.4xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.4xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.4xlarge#0,g6e.4xlarge#1",2048,g6e.4xlarge,32,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp2-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,32.56,5.622372110299232e-05,NVIDIA A10G,56.22372110299232,,"g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,32,32,204800.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,4.940579746917426,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,
,,,NVIDIA A10G,,,,2048,g5.48xlarge,32,32,,,,,,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp2-in2048-out512-bs32_32-20260130_053009,,ERROR,,,,,2,,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.48xlarge#0, TP=4, layers 1-40, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 2 (g6e.48xlarge#1, TP=4, layers 41-80, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44","g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,32,32,,llama3-70b,80,8,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,11.34,1.5131786394513027e-05,NVIDIA A10G,15.131786394513027,,"g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,32,32,102400.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053010,2048,SUCCESS,18.357236253249216,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,
32,27.4,3.048035739816374e-05,NVIDIA L40S,30.48035739816374,,"g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,32,32,819200.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,9.113337292905635,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,
32,54.78,6.093846636027043e-05,NVIDIA L40S,60.938466360270425,,"g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,32,32,409600.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,4.558332271369375,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.24xlarge#0, TP=4, layers 1-40, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44 | Stage 2 (g6e.24xlarge#1, TP=4, layers 41-80, size=40): underutilized GPU: segment uses 40 layers, but the minimum to reach 50% of GPU memory is 44 layers. Target memory=24.00GB (= 50% of 48.0GB), activation=1.51GB, per_layer=0.52GB ⇒ required_layers=ceil((24.00 - 1.51) / 0.52) = 44","g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,32,32,,llama3-70b,80,8,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.12xlarge#0,2048,g5.12xlarge,32,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.24xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.24xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,32,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp2-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.4xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.4xlarge#0,2048,g6e.4xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053011,2048,INFEASIBLE,,,,,1,1,aggregated
32,40.96,1.7558008729596158e-05,NVIDIA A100,17.558008729596157,,p4de.24xlarge#0,2048,p4de.24xlarge,32,32,,llama3-70b,80,4,1,1,512,1.0,1,2160.035714186494,0.3,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp4-in2048-out512-bs32_32-20260130_053011,2048,SUCCESS,15.820574078514356,162.00267856398702,648.0107142559481,0.4286623224999062,4,4,
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.24xlarge#0,2048,g6e.24xlarge,32,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp1-in2048-out512-bs32_32-20260130_053012,2048,INFEASIBLE,,,,,1,1,aggregated
