batch_size,cost_per_hour,cost_per_token,device_type,dollar_per_million_token,error,gpu_types,input_length,instance_family,max_batch_size,min_batch_size,min_network_throughput,model_name,num_decoder_layers,num_gpus,num_instances,num_pipeline_stages,output_length,pipeline_efficiency,pp_stages,raw_throughput_tokens_per_sec,real_world_efficiency,result_dir,sequence_length,status,throughput_per_dollar,throughput_per_gpu,throughput_tokens_per_sec,total_runtime_hours,tp_degree,tp_degrees,workload_phase
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=32.0GB. Max feasible layers=59 (max_required=31.93GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp4-20260204_231406,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA A100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p4de.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=80.0GB. Max feasible layers=74 (max_required=79.33GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",p4de.24xlarge#0,2048,p4de.24xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.48xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.48xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,11.34,1.5131786394513027e-05,NVIDIA A10G,15.131786394513027,,"g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,,32,102400.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,18.357236253249216,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.48xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.48xlarge#0,2048,g6e.48xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,54.78,1.9699389692406357e-05,NVIDIA L40S,19.699389692406356,,"g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,16,2,2,512,0.7200000000000001,2,3576.1275838587712,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp8-20260204_231406,2048,SUCCESS,14.100831655960107,48.277722382093415,772.4435581134946,0.35960915831336904,8,8,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.48xlarge#0,2048,g5.48xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.24xlarge#0,2048,g5.24xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.24xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.24xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,22.68,3.9163206222846e-05,NVIDIA A10G,39.163206222846,,"g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,,32,102400.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,7.09282524513366,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,aggregated
32,32.56,5.622372110299232e-05,NVIDIA A10G,56.22372110299232,,"g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,,32,204800.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,4.940579746917426,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.12xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.12xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.12xlarge#0,g6e.12xlarge#1",2048,g6e.12xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,65.16,6.086340749793019e-05,NVIDIA A10G,60.86340749793019,,"g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,,32,204800.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,1927.5098065911673,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,4.56395376461997,18.586701706414825,297.3872273026372,0.9340608885501872,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.24xlarge#0,2048,g5.24xlarge,,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp4-20260204_231406,2048,INFEASIBLE,,,,,4,4,aggregated
32,109.56,4.749937506124387e-05,NVIDIA L40S,47.49937506124387,,"g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,4152.751192314058,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,5.8480301565993615,40.04438649731413,640.7101839570261,0.4335466873059864,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.4xlarge#0,2048,g5.4xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,13.7,1.524017869908187e-05,NVIDIA L40S,15.24017869908187,,"g6e.12xlarge#0,g6e.12xlarge#1",2048,g6e.12xlarge,,32,819200.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp2-tp2-20260204_231406,2048,SUCCESS,18.22667458581127,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,aggregated
32,124.84,0.0001789451970545954,NVIDIA V100,178.94519705459538,,"p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,,32,102400.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1256.0458631198826,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,1.5523064175510057,24.223741645883447,193.78993316706757,1.433396323731139,2,2,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.12xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.12xlarge#0,2048,g6e.12xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.4xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.4xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.4xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.4xlarge#0,g5.4xlarge#1,g5.4xlarge#2,g5.4xlarge#3",2048,g5.4xlarge,,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp4-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,124.84,5.049366285543991e-05,NVIDIA V100,50.49366285543991,,"p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,,32,102400.0,llama3-70b,80,32,4,4,512,0.5142857142857142,4,4451.31847790642,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp8-20260204_231406,2048,SUCCESS,5.501240394721167,21.461714089905954,686.7748508769905,0.40446702062992557,8,8,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (p3dn.24xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (p3dn.24xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,65.16,3.367812298829339e-05,NVIDIA A10G,33.67812298829339,,"g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,,32,204800.0,llama3-70b,80,32,4,4,512,0.5142857142857142,4,3483.4131004151795,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp8-20260204_231406,2048,SUCCESS,8.248018390880457,16.79502744843033,537.4408783497705,0.5168527162107641,8,8,aggregated
32,22.68,2.1184500952318245e-05,NVIDIA A10G,21.184500952318245,,"g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,,32,102400.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,1927.5098065911673,0.3,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,13.112311609463722,18.586701706414825,297.3872273026372,0.9340608885501872,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.48xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.48xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.48xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,27.4,3.048035739816374e-05,NVIDIA L40S,30.48035739816374,,"g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,,32,819200.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp2-20260204_231406,2048,SUCCESS,9.113337292905635,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,aggregated
32,16.28,2.1723587522281492e-05,NVIDIA A10G,21.72358752228149,,"g5.24xlarge#0,g5.24xlarge#1",2048,g5.24xlarge,,32,204800.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,12.786920092865238,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.12xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.24xlarge#0,2048,g5.24xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.12xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.12xlarge#0,2048,g6e.12xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,62.42,6.769193747937186e-05,NVIDIA V100,67.69193747937186,,"p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,,32,102400.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,1185.8522112669311,0.29999999999999993,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,4.103557796117544,32.01800970420714,256.1440776336571,1.0844591073273286,4,4,aggregated
32,32.56,3.0413022531194096e-05,NVIDIA A10G,30.413022531194095,,"g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,,32,204800.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,1927.5098065911673,0.3,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,9.133514352046596,18.586701706414825,297.3872273026372,0.9340608885501872,4,4,aggregated
32,54.78,6.093846636027043e-05,NVIDIA L40S,60.938466360270425,,"g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,4,2,2,512,0.7200000000000001,2,1156.043712155622,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp2-20260204_231406,2048,SUCCESS,4.558332271369375,62.42636045640359,249.70544182561437,1.1124218028526913,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.12xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.12xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.12xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.12xlarge#0,g5.12xlarge#1,g5.12xlarge#2,g5.12xlarge#3",2048,g5.12xlarge,,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp4-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g5.12xlarge#0,2048,g5.12xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.4xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.4xlarge#0,2048,g6e.4xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,54.78,3.392812504374561e-05,NVIDIA L40S,33.92812504374561,,"g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,2076.375596157029,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,8.187242219239108,56.06214109623979,448.49712876991833,0.6193524104371232,4,4,aggregated
32,54.8,7.866214868404545e-05,NVIDIA L40S,78.66214868404545,,"g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp1-20260204_231406,2048,SUCCESS,3.5312762545236414,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,aggregated
32,16.29,1.73201775368366e-05,NVIDIA A10G,17.320177536836603,,g5.48xlarge#0,2048,g5.48xlarge,,32,,llama3-70b,80,8,1,1,512,1.0,1,870.8532751037949,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp8-20260204_231406,2048,SUCCESS,16.03781353782311,32.656997816392305,261.25598253113844,1.0632398733478576,8,8,aggregated
32,,,NVIDIA A100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p4de.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=80.0GB. Max feasible layers=35 (max_required=78.22GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",p4de.24xlarge#0,2048,p4de.24xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.4xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.4xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.4xlarge#0,g5.4xlarge#1",2048,g5.4xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.4xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (p3dn.24xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (p3dn.24xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (p3dn.24xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,54.8,4.2672500357429244e-05,NVIDIA L40S,42.67250035742924,,"g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,,32,819200.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,2312.087424311244,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,6.509526637789737,44.5902574688597,356.7220597508776,0.7786952619968841,2,2,aggregated
32,40.96,1.7558008729596158e-05,NVIDIA A100,17.558008729596157,,p4de.24xlarge#0,2048,p4de.24xlarge,,32,,llama3-70b,80,4,1,1,512,1.0,1,2160.035714186494,0.3,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp4-20260204_231406,2048,SUCCESS,15.820574078514356,162.00267856398702,648.0107142559481,0.4286623224999062,4,4,aggregated
32,13.68,1.9636828357623024e-05,NVIDIA L40S,19.636828357623024,,"g6e.4xlarge#0,g6e.4xlarge#1,g6e.4xlarge#2,g6e.4xlarge#3",2048,g6e.4xlarge,,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp4-tp1-20260204_231406,2048,SUCCESS,14.145755756425112,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.48xlarge#0,2048,g5.48xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,27.4,2.1336250178714622e-05,NVIDIA L40S,21.33625017871462,,"g6e.12xlarge#0,g6e.12xlarge#1,g6e.12xlarge#2,g6e.12xlarge#3",2048,g6e.12xlarge,,32,819200.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,2312.087424311244,0.3,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,13.019053275579473,44.5902574688597,356.7220597508776,0.7786952619968841,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.48xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.48xlarge#0,2048,g5.48xlarge,,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp1-tp4-20260204_231406,2048,INFEASIBLE,,,,,4,4,aggregated
32,109.56,8.531385290437862e-05,NVIDIA L40S,85.31385290437862,,"g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,2312.087424311244,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,3.2559516224066956,44.5902574688597,356.7220597508776,0.7786952619968841,2,2,aggregated
32,27.4,3.9331074342022725e-05,NVIDIA L40S,39.331074342022724,,"g6e.12xlarge#0,g6e.12xlarge#1,g6e.12xlarge#2,g6e.12xlarge#3",2048,g6e.12xlarge,,32,819200.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.12xlarge-pp4-tp1-20260204_231406,2048,SUCCESS,7.062552509047283,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.24xlarge#0,2048,g6e.24xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g5.12xlarge#0,2048,g5.12xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g5.12xlarge#0, TP=4, layers 1-80, size=80): memory overflow: required=42.76GB (activation=1.51GB + 80×per_layer=0.52GB) > GPU_mem=24.0GB. Max feasible layers=43 (max_required=23.68GB). per_layer breakdown: weights=0.44GB + kv_cache=0.08GB",g5.12xlarge#0,2048,g5.12xlarge,,32,,llama3-70b,80,4,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp1-tp4-20260204_231406,2048,INFEASIBLE,,,,,4,4,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (p3dn.24xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.48xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.48xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.48xlarge#0,g6e.48xlarge#1",2048,g6e.48xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.48xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=48.0GB. Max feasible layers=43 (max_required=47.36GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",g6e.48xlarge#0,2048,g6e.48xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,27.39,2.4428250031496847e-05,NVIDIA L40S,24.428250031496848,,g6e.48xlarge#0,2048,g6e.48xlarge,,32,,llama3-70b,80,4,1,1,512,1.0,1,1038.1877980785146,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp4-20260204_231406,2048,SUCCESS,11.371169748943204,77.86408485588859,311.45633942355437,0.8918674710294576,4,4,aggregated
32,32.58,4.347386249852155e-05,NVIDIA A10G,43.473862498521555,,"g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,,32,204800.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,963.7549032955836,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,6.389535270467959,26.02138238898076,208.1710591118461,1.3343726979288384,4,4,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.12xlarge#0, TP=2, layers 1-40, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB | Stage 2 (g5.12xlarge#1, TP=2, layers 41-80, size=40): memory overflow: required=44.27GB (activation=3.02GB + 40×per_layer=1.03GB) > GPU_mem=24.0GB. Max feasible layers=20 (max_required=23.64GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB","g5.12xlarge#0,g5.12xlarge#1",2048,g5.12xlarge,,32,,llama3-70b,80,4,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.12xlarge-pp2-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,109.56,0.00015726687974131423,NVIDIA L40S,157.26687974131423,,"g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,4,4,4,512,0.5142857142857142,4,1254.2570104030267,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp1-20260204_231406,2048,SUCCESS,1.766282756004888,48.378484686973884,193.51393874789554,1.4354406694168877,1,1,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g5.48xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.48xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,27.4,1.6970256045977174e-05,NVIDIA L40S,16.970256045977173,,"g6e.24xlarge#0,g6e.24xlarge#1",2048,g6e.24xlarge,,32,819200.0,llama3-70b,80,8,2,2,512,0.7200000000000001,2,2076.375596157029,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp2-tp4-20260204_231406,2048,SUCCESS,16.368508349267092,56.06214109623979,448.49712876991833,0.6193524104371232,4,4,aggregated
32,65.16,0.00011251651311643056,NVIDIA A10G,112.51651311643056,,"g5.48xlarge#0,g5.48xlarge#1,g5.48xlarge#2,g5.48xlarge#3",2048,g5.48xlarge,,32,204800.0,llama3-70b,80,8,4,4,512,0.5142857142857142,4,1042.645311034648,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp4-tp2-20260204_231406,2048,SUCCESS,2.4687734278642024,20.108159569953926,160.8652765596314,1.7267727611484127,2,2,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=2, layers 1-80, size=80): memory overflow: required=85.52GB (activation=3.02GB + 80×per_layer=1.03GB) > GPU_mem=32.0GB. Max feasible layers=28 (max_required=31.89GB). per_layer breakdown: weights=0.88GB + kv_cache=0.16GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,,32,,llama3-70b,80,2,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp2-20260204_231406,2048,INFEASIBLE,,,,,2,2,aggregated
32,,,NVIDIA A10G,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=4. Reasons: Stage 1 (g5.24xlarge#0, TP=1, layers 1-20, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g5.24xlarge#1, TP=1, layers 21-40, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 3 (g5.24xlarge#2, TP=1, layers 41-60, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 4 (g5.24xlarge#3, TP=1, layers 61-80, size=20): memory overflow: required=47.29GB (activation=6.04GB + 20×per_layer=2.06GB) > GPU_mem=24.0GB. Max feasible layers=8 (max_required=22.54GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g5.24xlarge#0,g5.24xlarge#1,g5.24xlarge#2,g5.24xlarge#3",2048,g5.24xlarge,,32,,llama3-70b,80,4,4,,512,,4,,,large/in2048-out512-bs32/eval_family-g5.24xlarge-pp4-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (g6e.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",g6e.24xlarge#0,2048,g6e.24xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,32.58,2.405580213449528e-05,NVIDIA A10G,24.05580213449528,,"g5.48xlarge#0,g5.48xlarge#1",2048,g5.48xlarge,,32,204800.0,llama3-70b,80,16,2,2,512,0.7200000000000001,2,1741.7065502075898,0.3,large/in2048-out512-bs32/eval_family-g5.48xlarge-pp2-tp8-20260204_231406,2048,SUCCESS,11.547225747232641,23.513038427802464,376.2086148448394,0.7383610231582345,8,8,aggregated
32,27.39,1.4183560578532578e-05,NVIDIA L40S,14.183560578532578,,g6e.48xlarge#0,2048,g6e.48xlarge,,32,,llama3-70b,80,8,1,1,512,1.0,1,1788.0637919293856,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp1-tp8-20260204_231406,2048,SUCCESS,19.584488411055702,67.05239219735196,536.4191375788157,0.5178371879712514,8,8,aggregated
32,31.21,2.596816946851195e-05,NVIDIA V100,25.96816946851195,,p3dn.24xlarge#0,2048,p3dn.24xlarge,,32,,llama3-70b,80,8,1,1,512,1.0,1,1112.829619476605,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp8-20260204_231406,2048,SUCCESS,10.696856323068937,41.73111073037269,333.84888584298153,0.832046442438704,8,8,aggregated
32,109.56,2.7579145569368904e-05,NVIDIA L40S,27.579145569368904,,"g6e.48xlarge#0,g6e.48xlarge#1,g6e.48xlarge#2,g6e.48xlarge#3",2048,g6e.48xlarge,,32,409600.0,llama3-70b,80,32,4,4,512,0.5142857142857142,4,7152.2551677175425,0.3,large/in2048-out512-bs32/eval_family-g6e.48xlarge-pp4-tp8-20260204_231406,2048,SUCCESS,10.072022611400074,34.484087415781005,1103.4907973049922,0.25172641081935837,8,8,aggregated
32,54.8,2.3758358464368054e-05,NVIDIA L40S,23.758358464368055,,"g6e.24xlarge#0,g6e.24xlarge#1,g6e.24xlarge#2,g6e.24xlarge#3",2048,g6e.24xlarge,,32,819200.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,4152.751192314058,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,11.691791678047922,40.04438649731413,640.7101839570261,0.4335466873059864,4,4,aggregated
32,62.42,3.606690203959993e-05,NVIDIA V100,36.06690203959993,,"p3dn.24xlarge#0,p3dn.24xlarge#1",2048,p3dn.24xlarge,,32,102400.0,llama3-70b,80,16,2,2,512,0.7200000000000001,2,2225.65923895321,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp2-tp8-20260204_231406,2048,SUCCESS,7.701736552609636,30.04639972586834,480.74239561389345,0.5778100294713222,8,8,aggregated
32,,,NVIDIA V100,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=1. Reasons: Stage 1 (p3dn.24xlarge#0, TP=1, layers 1-80, size=80): memory overflow: required=171.04GB (activation=6.04GB + 80×per_layer=2.06GB) > GPU_mem=32.0GB. Max feasible layers=12 (max_required=30.79GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB",p3dn.24xlarge#0,2048,p3dn.24xlarge,,32,,llama3-70b,80,1,1,,512,,1,,,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp1-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
32,40.96,9.39992626541533e-06,NVIDIA A100,9.39992626541533,,p4de.24xlarge#0,2048,p4de.24xlarge,,32,,llama3-70b,80,8,1,1,512,1.0,1,4034.7046194888626,0.3,large/in2048-out512-bs32/eval_family-p4de.24xlarge-pp1-tp8-20260204_231406,2048,SUCCESS,29.55105922477194,151.30142323083234,1210.4113858466587,0.22949038733924146,8,8,aggregated
32,13.7,1.221858435310357e-05,NVIDIA L40S,12.21858435310357,,g6e.24xlarge#0,2048,g6e.24xlarge,,32,,llama3-70b,80,4,1,1,512,1.0,1,1038.1877980785146,0.3,large/in2048-out512-bs32/eval_family-g6e.24xlarge-pp1-tp4-20260204_231406,2048,SUCCESS,22.73403937398207,77.86408485588859,311.45633942355437,0.8918674710294576,4,4,aggregated
32,124.84,9.476871247112062e-05,NVIDIA V100,94.76871247112062,,"p3dn.24xlarge#0,p3dn.24xlarge#1,p3dn.24xlarge#2,p3dn.24xlarge#3",2048,p3dn.24xlarge,,32,102400.0,llama3-70b,80,16,4,4,512,0.5142857142857142,4,2371.7044225338623,0.3,large/in2048-out512-bs32/eval_family-p3dn.24xlarge-pp4-tp4-20260204_231406,2048,SUCCESS,2.931112711512531,22.870006931576523,365.92011090522436,0.7591213751291302,4,4,aggregated
32,,,NVIDIA L40S,,"INFEASIBLE placement. Context: model_layers=80, phase=aggregated, seq_len=2048, output_len=512, batch=32, pp_stages=2. Reasons: Stage 1 (g6e.4xlarge#0, TP=1, layers 1-40, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB | Stage 2 (g6e.4xlarge#1, TP=1, layers 41-80, size=40): memory overflow: required=88.54GB (activation=6.04GB + 40×per_layer=2.06GB) > GPU_mem=48.0GB. Max feasible layers=20 (max_required=47.29GB). per_layer breakdown: weights=1.75GB + kv_cache=0.31GB","g6e.4xlarge#0,g6e.4xlarge#1",2048,g6e.4xlarge,,32,,llama3-70b,80,2,2,,512,,2,,,large/in2048-out512-bs32/eval_family-g6e.4xlarge-pp2-tp1-20260204_231406,2048,INFEASIBLE,,,,,1,1,aggregated
